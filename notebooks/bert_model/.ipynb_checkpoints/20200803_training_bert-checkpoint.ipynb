{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "py37_env",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "20200803_training_bert-checkpoint.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alamhanz/the-one-with-friends/blob/master/notebooks/bert_model/.ipynb_checkpoints/20200803_training_bert-checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igHEEXYHmlYE"
      },
      "source": [
        "# Friends Classification Text\n",
        "\n",
        "This is classification of dialogue in FRIENDS TV Series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vQNk71SmlYF"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feSlDelRmlYN"
      },
      "source": [
        "PATH_DATA = '../../data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O2kF4PCmlYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uw8Ze0xmlYZ"
      },
      "source": [
        "## Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9t1HvjKmlYa"
      },
      "source": [
        "df_dlg_seen = pd.read_csv('../../data/friends_seen_season.csv')\n",
        "df_dlg_unseen = pd.read_csv('../../data/friends_unseen_season.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdUJrkkbmlYf"
      },
      "source": [
        "data_train = df_dlg_seen[df_dlg_seen.is_train == True][['text','label']]\n",
        "data_test = df_dlg_seen[df_dlg_seen.is_train == False][['text','label']]\n",
        "data_unseen = df_dlg_unseen[['text','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy3qaQgimlYj",
        "outputId": "8a7722f8-b177-400b-a7b2-5aa719399617"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35123, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLD_YQEImlYz",
        "outputId": "de0588b1-fe15-4900-8df9-f5af89d026e9"
      },
      "source": [
        "data_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11708, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSIPl62mlY3",
        "outputId": "5728898f-df8c-4751-8c6b-eb079257927a"
      },
      "source": [
        "data_unseen.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11207, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFbcLiA1mlY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsE2bxexmlY-"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4nlmCbxmlY_"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "bert_token = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "MAX_LEN = 30\n",
        "BATCH_SIZE = 30\n",
        "NUMB_CLASS = 7\n",
        "\n",
        "# Tokenizer use bert\n",
        "def text_to_feature(x,tokenizer = bert_token):\n",
        "    text_token = tokenizer.encode_plus(\n",
        "                    x,                      \n",
        "                    add_special_tokens = True, # add [CLS], [SEP]\n",
        "                    max_length = MAX_LEN, # max length of the text that can go to BERT\n",
        "                    pad_to_max_length = True, # add [PAD] tokens\n",
        "                    return_attention_mask = True,\n",
        "                    truncation=True,# add attention mask to not focus on pad tokens\n",
        "                  )\n",
        "    return text_token\n",
        "\n",
        "# map to the expected input to TFBertForSequenceClassification, see here \n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "    }, label\n",
        "\n",
        "\n",
        "def label_encode(i, numb_cls):\n",
        "    X = np.zeros(numb_cls)\n",
        "    X[i] = 1\n",
        "    return list(X)\n",
        "\n",
        "def encode_dataset(ds, limit=-1):\n",
        "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "    if (limit > 0):\n",
        "        ds = ds.head(limit)\n",
        "    \n",
        "    for text, label in ds.values:\n",
        "        bert_input = text_to_feature(text)\n",
        "        input_ids_list.append(bert_input['input_ids'])\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append(label_encode(label, NUMB_CLASS))\n",
        "        \n",
        "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYTt6XkemlZB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLvnLn5WmlZD",
        "outputId": "420e4b14-5789-48c4-afbe-9a518134af4a"
      },
      "source": [
        "L1 = time.time()\n",
        "input_train = encode_dataset(data_train).batch(BATCH_SIZE)\n",
        "finish_time = str(round((time.time()-L1)/60,3))\n",
        "print('done in '+finish_time)\n",
        "\n",
        "L1 = time.time()\n",
        "input_test = encode_dataset(data_test).batch(BATCH_SIZE)\n",
        "finish_time = str(round((time.time()-L1)/60,3))\n",
        "print('done in '+finish_time)\n",
        "\n",
        "L1 = time.time()\n",
        "input_unseen = encode_dataset(data_unseen).batch(BATCH_SIZE)\n",
        "finish_time = str(round((time.time()-L1)/60,3))\n",
        "print('done in '+finish_time)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done in 0.415\n",
            "done in 0.126\n",
            "done in 0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZqIDEZ8mlZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hgZ2zfDmlZK"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP2erusbmlZL"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxCpY1WmlZP",
        "outputId": "6efc0798-3681-4993-f25c-de25df2a939f"
      },
      "source": [
        "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
        "LR = 3e-5\n",
        "EPOCHS = 25\n",
        "\n",
        "# model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUMB_CLASS)\n",
        "# classifier Adam recommended\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LR, epsilon=1e-08)\n",
        "\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metric = tf.keras.metrics.CategoricalCrossentropy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_75', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONuND8jAmlZR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fieKr8OUmlZT",
        "outputId": "7238c46f-b4a9-445c-d6c1-8d5bfdc24e2b"
      },
      "source": [
        "bert_history = model.fit(input_train, epochs=EPOCHS, validation_data=input_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  15/1171 [..............................] - ETA: 2:03:39 - loss: 6.8291 - accuracy: 6.8291"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOK3PwWvmlZW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxATTE0LmlZY",
        "outputId": "aa5c20ff-2b64-4c0f-edf4-2dcc88d483a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e1243bd22c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eNcbGHWmlZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}