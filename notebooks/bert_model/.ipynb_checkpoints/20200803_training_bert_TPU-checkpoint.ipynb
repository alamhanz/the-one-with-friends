{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5_fZUzo02xP"
   },
   "source": [
    "# Friends Classification Text\n",
    "\n",
    "This is classification of dialogue in FRIENDS TV Series.\n",
    "\n",
    "<!-- https://stackoverflow.com/questions/61000500/tensorflow-keras-bert-multiclass-text-classification-accuracy -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03Lpsmju02xR",
    "outputId": "729cb2e1-2326-4654-da9e-3facd4921f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nplks9aMi_aH"
   },
   "outputs": [],
   "source": [
    "# help(drive.mount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z6n6vof_02xV"
   },
   "outputs": [],
   "source": [
    "# PATH_DATA = 'gdrive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvelJ79e02xY",
    "outputId": "d017b6ae-6106-451a-9f99-6e6f6ef824ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'gdrive', 'sample_data']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g_TiNQM1LvY"
   },
   "source": [
    "## Checking Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWQMw6cQ1NtR",
    "outputId": "1c7a283d-0e73-409b-8170-d008ab0f60d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU address is grpc://10.103.117.34:8470\n"
     ]
    }
   ],
   "source": [
    "if 'COLAB_TPU_ADDR' not in os.environ:\n",
    "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
    "else:\n",
    "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "  print ('TPU address is', tpu_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uQwZJZB61Nwm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lMeZ84P02xb"
   },
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Nfyykbz1wZv",
    "outputId": "13b39367-505a-4952-e94a-9f7c0dfacd90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Repository/Data\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/My Drive/Repository/Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCCTJo2o1dDF",
    "outputId": "0d76ccce-34a4-47f5-9815-a2364d8bfcb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends_seen_season.csv', 'friends_unseen_season.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dHeMuViP02xb"
   },
   "outputs": [],
   "source": [
    "df_dlg_seen = pd.read_csv('friends_seen_season.csv')\n",
    "df_dlg_unseen = pd.read_csv('friends_unseen_season.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y_VRmC1602xe"
   },
   "outputs": [],
   "source": [
    "data_train = df_dlg_seen[(df_dlg_seen.is_train == True)&(df_dlg_seen.label!=0)][['text','label']]\n",
    "data_train['label'] = data_train['label']-1\n",
    "data_test = df_dlg_seen[(df_dlg_seen.is_train == False)&(df_dlg_seen.label!=0)][['text','label']]\n",
    "data_test['label'] = data_test['label']-1\n",
    "data_unseen = df_dlg_unseen[(df_dlg_unseen.label!=0)][['text','label']]\n",
    "data_unseen['label'] = data_unseen['label']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tt421RVa02xh",
    "outputId": "03de5b03-ec4f-4f0f-8656-399e7cb20cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25091, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnHpAS_J02xk",
    "outputId": "4ab46916-557f-4c8a-c7d8-e19ff5f1478c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8320, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1kNu3H602xo",
    "outputId": "3233fad8-8882-47d7-ed3b-4d0807ed4c05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8159, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWx_PNZHay70",
    "outputId": "b877f337-b9ed-4c6e-e07c-8c4d0e0d7428"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.180889\n",
       "0    0.178005\n",
       "3    0.170793\n",
       "2    0.166466\n",
       "4    0.161779\n",
       "5    0.142067\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX = data_test.label.value_counts()\n",
    "XX/XX.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L1J6bjri02xr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KQGGRST02xu"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SBGtoVam2ION"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PcAEmNSn02xu"
   },
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# bert_token = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# MAX_LEN = 25\n",
    "# NUMB_CLASS = 7\n",
    "\n",
    "# # Tokenizer use bert\n",
    "# def text_to_feature(x,tokenizer = bert_token):\n",
    "#     text_token = tokenizer.encode_plus(\n",
    "#                     x,                      \n",
    "#                     add_special_tokens = True, # add [CLS], [SEP]\n",
    "#                     max_length = MAX_LEN, # max length of the text that can go to BERT\n",
    "#                     pad_to_max_length = True, # add [PAD] tokens\n",
    "#                     return_attention_mask = True,\n",
    "#                     truncation=True,# add attention mask to not focus on pad tokens\n",
    "#                   )\n",
    "#     return text_token\n",
    "\n",
    "# # map to the expected input to TFBertForSequenceClassification, see here \n",
    "# def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "#     return {\n",
    "#       \"input_ids\": input_ids,\n",
    "#       \"token_type_ids\": token_type_ids,\n",
    "#       \"attention_mask\": attention_masks,\n",
    "#     }, label\n",
    "\n",
    "\n",
    "# def label_encode(i, numb_cls):\n",
    "#     X = np.zeros(numb_cls)\n",
    "#     X[i] = 1\n",
    "#     return list(X)\n",
    "\n",
    "# def encode_dataset(ds, limit=-1):\n",
    "#     # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "#     input_ids_list = []\n",
    "#     token_type_ids_list = []\n",
    "#     attention_mask_list = []\n",
    "#     label_list = []\n",
    "#     if (limit > 0):\n",
    "#         ds = ds.head(limit)\n",
    "    \n",
    "#     for text, label in ds.values:\n",
    "#         bert_input = text_to_feature(text)\n",
    "#         input_ids_list.append(bert_input['input_ids'])\n",
    "#         token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "#         attention_mask_list.append(bert_input['attention_mask'])\n",
    "        \n",
    "#     return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C_CcwBtOmxzy"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_token = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenizer use bert\n",
    "def text_to_feature(x,max_seq,tokenizer = bert_token):\n",
    "    text_token = tokenizer.encode_plus(\n",
    "                    x,                      \n",
    "                    add_special_tokens = True, # add [CLS], [SEP]\n",
    "                    max_length = max_seq, # max length of the text that can go to BERT\n",
    "                    pad_to_max_length = True, # add [PAD] tokens\n",
    "                    return_attention_mask = True,\n",
    "                    truncation=True,# add attention mask to not focus on pad tokens\n",
    "\n",
    "                    return_tensors='tf',\n",
    "                  )\n",
    "    return text_token\n",
    "\n",
    "def label_encode(i, numb_cls):\n",
    "    X = np.zeros(numb_cls)\n",
    "    X[i] = 1\n",
    "    return list(X)\n",
    "\n",
    "def encode_dataset(ds, max_seq_len, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.head(limit)\n",
    "    \n",
    "    for text, label in ds.values:\n",
    "        bert_input = text_to_feature(text,max_seq = max_seq_len)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append(label_encode(label, NUMB_CLASS))\n",
    "\n",
    "    input_ids = tf.convert_to_tensor(input_ids_list)\n",
    "    attention_masks = tf.convert_to_tensor(attention_mask_list)\n",
    "    token_type_ids = tf.convert_to_tensor(token_type_ids_list)\n",
    "\n",
    "    ids = tf.reshape(input_ids, (-1, max_seq_len))\n",
    "    print(\"Input ids shape: \", ids.shape)\n",
    "    masks = tf.reshape(attention_masks, (-1, max_seq_len))\n",
    "    print(\"Input Masks shape: \", masks.shape)\n",
    "    token_types = tf.reshape(token_type_ids, (-1, max_seq_len))\n",
    "    print(\"Token type ids shape: \", token_types.shape)\n",
    "\n",
    "    ids=ids.numpy()\n",
    "    masks = masks.numpy()\n",
    "    token_types = token_types.numpy()\n",
    "    y = np.array(label_list)\n",
    "        \n",
    "    return [ids, masks, token_types, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y1vS0VkKmOZR"
   },
   "outputs": [],
   "source": [
    "# ALL_TEST = encode_dataset(data_test, MAX_LEN)\n",
    "# ALL_TEST[:3],ALL_TEST[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5wougnTMpWxj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYqyu2RK02x6"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JbdOHXqA02x6"
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SQBszBZQ3Yzd"
   },
   "outputs": [],
   "source": [
    "# def create_model(L_RATE, NUMB_CLASS_TARGET):\n",
    "#   model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUMB_CLASS_TARGET)\n",
    "#   optimizer = tf.keras.optimizers.Adam(learning_rate=L_RATE, epsilon=1e-08)\n",
    "\n",
    "#   loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#   metric = tf.keras.metrics.CategoricalCrossentropy('accuracy')\n",
    "\n",
    "#   model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "#   return model\n",
    "\n",
    "\n",
    "def create_model(L_RATE,max_seq_len,NUMB_CLASS_TARGET):\n",
    "  base_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', trainable=True, num_labels=NUMB_CLASS_TARGET)\n",
    "\n",
    "  input_ids_layer = Input(shape=(max_seq_len, ), dtype=np.int32)\n",
    "  input_mask_layer = Input(shape=(max_seq_len, ), dtype=np.int32)\n",
    "  input_token_type_layer = Input(shape=(max_seq_len,), dtype=np.int32)\n",
    "\n",
    "  bert_layer = base_model([input_ids_layer, input_mask_layer, input_token_type_layer])[0]\n",
    "  flat_layer = Flatten()(bert_layer)\n",
    "  # dropout_1= Dropout(0.1)(flat_layer)\n",
    "  # dense_1 = Dense(NUMB_CLASS_TARGET, activation='relu', \n",
    "  #                 bias_regularizer=regularizers.l2(0.01),\n",
    "  #                 activity_regularizer=regularizers.l2(0.02))(dropout_1)\n",
    "\n",
    "  # dropout_2= Dropout(0.1)(dense_1)\n",
    "  # dense_2 = Dense(2*NUMB_CLASS_TARGET, activation='relu',\n",
    "  #                 bias_regularizer=regularizers.l2(0.01),\n",
    "  #                 activity_regularizer=regularizers.l2(0.01))(dropout_2)\n",
    "\n",
    "  dense_output = Dense(NUMB_CLASS_TARGET, activation='softmax',\n",
    "                  bias_regularizer=regularizers.l2(0.02))(flat_layer)\n",
    "\n",
    "  model_ = Model(inputs=[input_ids_layer, input_mask_layer, input_token_type_layer], outputs=dense_output)\n",
    "\n",
    "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=L_RATE,\n",
    "    decay_steps=10000000,\n",
    "    decay_rate=0.15)\n",
    "  \n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, epsilon=5e-08)\n",
    "  model_.compile(optimizer=optimizer,\n",
    "              loss=['categorical_crossentropy'],\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  #loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  #metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "  #model_.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "  return model_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Kw1ZtZxdmaKX"
   },
   "outputs": [],
   "source": [
    "# mm = create_model(LR, MAX_LEN , NUMB_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZVc0jotHmiCm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHof8BHk7FJF",
    "outputId": "4dd04629-d9f4-4cee-b891-31c565b0c5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SKvzYkhC7CHN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KA1k9kZc290S",
    "outputId": "01d43e7d-f920-427c-a028-82e6e7303596"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TPU\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.103.117.34:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.103.117.34:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "LR = 1e-5\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 250\n",
    "MAX_LEN = 8\n",
    "NUMB_CLASS = 6\n",
    "\n",
    "use_tpu = 'COLAB_TPU_ADDR' in os.environ\n",
    "if use_tpu:\n",
    "    print('USING TPU')\n",
    "    # Create distribution strategy\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        bert_model = create_model(LR, MAX_LEN , NUMB_CLASS)\n",
    "else:\n",
    "    print('NOT USING TPU')\n",
    "    bert_model = create_model(LR, MAX_LEN, NUMB_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lG29JpA4JUu",
    "outputId": "05a32ed9-f63e-414f-eccf-062d32041c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_sequence_classifica ((None, 6),)         109486854   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6)            0           tf_bert_for_sequence_classificati\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 6)            42          flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,486,896\n",
      "Trainable params: 109,486,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Gl_n5r4Jtf1J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fxNkvSpJ7z0j"
   },
   "outputs": [],
   "source": [
    "# XX = encode_dataset(data_test)\n",
    "# YY = data_test['label'].tolist()\n",
    "# ALL_TEST = convert_inputs_to_tf_dataset(XX,YY, MAX_LEN)\n",
    "# ALL_TEST[:3],ALL_TEST[3].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9hiqMVc7z61",
    "outputId": "65940a1a-151c-42f9-a659-2acd8f5456fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids shape:  (25091, 8)\n",
      "Input Masks shape:  (25091, 8)\n",
      "Token type ids shape:  (25091, 8)\n",
      "done in 0.383\n",
      "Input ids shape:  (8320, 8)\n",
      "Input Masks shape:  (8320, 8)\n",
      "Token type ids shape:  (8320, 8)\n",
      "done in 0.146\n",
      "Input ids shape:  (8159, 8)\n",
      "Input Masks shape:  (8159, 8)\n",
      "Token type ids shape:  (8159, 8)\n",
      "done in 0.132\n"
     ]
    }
   ],
   "source": [
    "L1 = time.time()\n",
    "all_train = encode_dataset(data_train, MAX_LEN)\n",
    "X_train,y_train = all_train[:3],all_train[3]\n",
    "finish_time = str(round((time.time()-L1)/60,3))\n",
    "print('done in '+finish_time)\n",
    "\n",
    "L1 = time.time()\n",
    "all_val = encode_dataset(data_test, MAX_LEN)\n",
    "X_val,y_val = all_val[:3],all_val[3]\n",
    "finish_time = str(round((time.time()-L1)/60,3))\n",
    "print('done in '+finish_time)\n",
    "\n",
    "L1 = time.time()\n",
    "all_unseen = encode_dataset(data_unseen, MAX_LEN)\n",
    "X_unseen,y_unseen = all_unseen[:3],all_unseen[3]\n",
    "finish_time = str(round((time.time()-L1)/60,3))\n",
    "print('done in '+finish_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GCJGfzYFto1I"
   },
   "outputs": [],
   "source": [
    "# X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pUXHRi-nto7S"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-5, epsilon=1e-08)\n",
    "# bert_model.compile(optimizer=optimizer,\n",
    "#               loss=['categorical_crossentropy'],\n",
    "#               metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BdhEg9g02yC",
    "outputId": "102a02d0-d8be-4dcb-900f-088f7c2aa39f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/101 [..............................] - ETA: 30:32 - loss: 1.8063 - accuracy: 0.1540  WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0118s vs `on_train_batch_end` time: 0.0783s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0118s vs `on_train_batch_end` time: 0.0783s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - ETA: 0s - loss: 1.7891 - accuracy: 0.1878WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_test_batch_end` time: 0.0237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_test_batch_end` time: 0.0237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 89s 880ms/step - loss: 1.7891 - accuracy: 0.1878 - val_loss: 1.7733 - val_accuracy: 0.2168\n",
      "Epoch 2/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 1.7607 - accuracy: 0.2251 - val_loss: 1.7466 - val_accuracy: 0.2446\n",
      "Epoch 3/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 1.7292 - accuracy: 0.2579 - val_loss: 1.7372 - val_accuracy: 0.2555\n",
      "Epoch 4/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 1.7023 - accuracy: 0.2739 - val_loss: 1.7341 - val_accuracy: 0.2555\n",
      "Epoch 5/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 1.6752 - accuracy: 0.2956 - val_loss: 1.7390 - val_accuracy: 0.2585\n",
      "Epoch 6/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 1.6358 - accuracy: 0.3282 - val_loss: 1.7484 - val_accuracy: 0.2660\n",
      "Epoch 7/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 1.5944 - accuracy: 0.3524 - val_loss: 1.7516 - val_accuracy: 0.2685\n",
      "Epoch 8/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 1.5444 - accuracy: 0.3820 - val_loss: 1.7839 - val_accuracy: 0.2686\n",
      "Epoch 9/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 1.4861 - accuracy: 0.4107 - val_loss: 1.8224 - val_accuracy: 0.2728\n",
      "Epoch 10/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 1.4207 - accuracy: 0.4415 - val_loss: 1.8735 - val_accuracy: 0.2601\n",
      "Epoch 11/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 1.3613 - accuracy: 0.4654 - val_loss: 1.9036 - val_accuracy: 0.2647\n",
      "Epoch 12/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 1.2877 - accuracy: 0.4980 - val_loss: 1.9802 - val_accuracy: 0.2697\n",
      "Epoch 13/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 1.2159 - accuracy: 0.5304 - val_loss: 2.0115 - val_accuracy: 0.2637\n",
      "Epoch 14/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 1.1485 - accuracy: 0.5578 - val_loss: 2.1081 - val_accuracy: 0.2669\n",
      "Epoch 15/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 1.0862 - accuracy: 0.5827 - val_loss: 2.1562 - val_accuracy: 0.2673\n",
      "Epoch 16/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 1.0205 - accuracy: 0.6075 - val_loss: 2.2547 - val_accuracy: 0.2639\n",
      "Epoch 17/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.9592 - accuracy: 0.6362 - val_loss: 2.3360 - val_accuracy: 0.2657\n",
      "Epoch 18/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.9128 - accuracy: 0.6553 - val_loss: 2.3718 - val_accuracy: 0.2627\n",
      "Epoch 19/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.8567 - accuracy: 0.6743 - val_loss: 2.4793 - val_accuracy: 0.2639\n",
      "Epoch 20/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.8094 - accuracy: 0.6954 - val_loss: 2.5628 - val_accuracy: 0.2666\n",
      "Epoch 21/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.7699 - accuracy: 0.7087 - val_loss: 2.5952 - val_accuracy: 0.2626\n",
      "Epoch 22/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.7272 - accuracy: 0.7251 - val_loss: 2.6964 - val_accuracy: 0.2632\n",
      "Epoch 23/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.6962 - accuracy: 0.7372 - val_loss: 2.7681 - val_accuracy: 0.2632\n",
      "Epoch 24/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.6644 - accuracy: 0.7501 - val_loss: 2.8337 - val_accuracy: 0.2642\n",
      "Epoch 25/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.6355 - accuracy: 0.7631 - val_loss: 2.8855 - val_accuracy: 0.2639\n",
      "Epoch 26/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.6053 - accuracy: 0.7739 - val_loss: 2.9907 - val_accuracy: 0.2639\n",
      "Epoch 27/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.5698 - accuracy: 0.7894 - val_loss: 3.0729 - val_accuracy: 0.2589\n",
      "Epoch 28/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.5514 - accuracy: 0.7949 - val_loss: 3.1121 - val_accuracy: 0.2601\n",
      "Epoch 29/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.5320 - accuracy: 0.7998 - val_loss: 3.1930 - val_accuracy: 0.2582\n",
      "Epoch 30/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.5104 - accuracy: 0.8136 - val_loss: 3.2637 - val_accuracy: 0.2595\n",
      "Epoch 31/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.4958 - accuracy: 0.8165 - val_loss: 3.3199 - val_accuracy: 0.2614\n",
      "Epoch 32/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.4708 - accuracy: 0.8241 - val_loss: 3.3351 - val_accuracy: 0.2546\n",
      "Epoch 33/250\n",
      "101/101 [==============================] - 11s 109ms/step - loss: 0.4552 - accuracy: 0.8314 - val_loss: 3.3770 - val_accuracy: 0.2635\n",
      "Epoch 34/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.4435 - accuracy: 0.8332 - val_loss: 3.4829 - val_accuracy: 0.2625\n",
      "Epoch 35/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.4323 - accuracy: 0.8401 - val_loss: 3.4955 - val_accuracy: 0.2615\n",
      "Epoch 36/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.4190 - accuracy: 0.8459 - val_loss: 3.5713 - val_accuracy: 0.2582\n",
      "Epoch 37/250\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 0.4055 - accuracy: 0.8515 - val_loss: 3.6252 - val_accuracy: 0.2617\n",
      "Epoch 38/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.3979 - accuracy: 0.8529 - val_loss: 3.6733 - val_accuracy: 0.2612\n",
      "Epoch 39/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.3818 - accuracy: 0.8587 - val_loss: 3.6676 - val_accuracy: 0.2633\n",
      "Epoch 40/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.3745 - accuracy: 0.8607 - val_loss: 3.7679 - val_accuracy: 0.2629\n",
      "Epoch 41/250\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 0.3659 - accuracy: 0.8624 - val_loss: 3.7582 - val_accuracy: 0.2620\n",
      "Epoch 42/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.3602 - accuracy: 0.8675 - val_loss: 3.8001 - val_accuracy: 0.2643\n",
      "Epoch 43/250\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.3507 - accuracy: 0.8698 - val_loss: 3.8451 - val_accuracy: 0.2606\n",
      "Epoch 44/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.3401 - accuracy: 0.8719 - val_loss: 3.8839 - val_accuracy: 0.2590\n",
      "Epoch 45/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.3310 - accuracy: 0.8756 - val_loss: 3.9589 - val_accuracy: 0.2603\n",
      "Epoch 46/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.3256 - accuracy: 0.8781 - val_loss: 3.9857 - val_accuracy: 0.2608\n",
      "Epoch 47/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.3131 - accuracy: 0.8806 - val_loss: 4.0039 - val_accuracy: 0.2641\n",
      "Epoch 48/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.3115 - accuracy: 0.8825 - val_loss: 4.0416 - val_accuracy: 0.2567\n",
      "Epoch 49/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.3030 - accuracy: 0.8865 - val_loss: 4.0669 - val_accuracy: 0.2595\n",
      "Epoch 50/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.3059 - accuracy: 0.8841 - val_loss: 4.1114 - val_accuracy: 0.2630\n",
      "Epoch 51/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2971 - accuracy: 0.8860 - val_loss: 4.1205 - val_accuracy: 0.2657\n",
      "Epoch 52/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2927 - accuracy: 0.8878 - val_loss: 4.1682 - val_accuracy: 0.2629\n",
      "Epoch 53/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2895 - accuracy: 0.8904 - val_loss: 4.1970 - val_accuracy: 0.2626\n",
      "Epoch 54/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2847 - accuracy: 0.8894 - val_loss: 4.2486 - val_accuracy: 0.2667\n",
      "Epoch 55/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2829 - accuracy: 0.8919 - val_loss: 4.2332 - val_accuracy: 0.2617\n",
      "Epoch 56/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2822 - accuracy: 0.8910 - val_loss: 4.2112 - val_accuracy: 0.2620\n",
      "Epoch 57/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2808 - accuracy: 0.8915 - val_loss: 4.2633 - val_accuracy: 0.2614\n",
      "Epoch 58/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2785 - accuracy: 0.8917 - val_loss: 4.2573 - val_accuracy: 0.2649\n",
      "Epoch 59/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2737 - accuracy: 0.8939 - val_loss: 4.3506 - val_accuracy: 0.2636\n",
      "Epoch 60/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2714 - accuracy: 0.8960 - val_loss: 4.3270 - val_accuracy: 0.2654\n",
      "Epoch 61/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2654 - accuracy: 0.8974 - val_loss: 4.3690 - val_accuracy: 0.2665\n",
      "Epoch 62/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2630 - accuracy: 0.8978 - val_loss: 4.3995 - val_accuracy: 0.2637\n",
      "Epoch 63/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2599 - accuracy: 0.8991 - val_loss: 4.4285 - val_accuracy: 0.2650\n",
      "Epoch 64/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2549 - accuracy: 0.9010 - val_loss: 4.4844 - val_accuracy: 0.2649\n",
      "Epoch 65/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2539 - accuracy: 0.9011 - val_loss: 4.4929 - val_accuracy: 0.2631\n",
      "Epoch 66/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2567 - accuracy: 0.8982 - val_loss: 4.4666 - val_accuracy: 0.2631\n",
      "Epoch 67/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2496 - accuracy: 0.9011 - val_loss: 4.5133 - val_accuracy: 0.2623\n",
      "Epoch 68/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2506 - accuracy: 0.9006 - val_loss: 4.5686 - val_accuracy: 0.2613\n",
      "Epoch 69/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2490 - accuracy: 0.9011 - val_loss: 4.5472 - val_accuracy: 0.2635\n",
      "Epoch 70/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2411 - accuracy: 0.9030 - val_loss: 4.6413 - val_accuracy: 0.2654\n",
      "Epoch 71/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2472 - accuracy: 0.9012 - val_loss: 4.5362 - val_accuracy: 0.2601\n",
      "Epoch 72/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2414 - accuracy: 0.9039 - val_loss: 4.6071 - val_accuracy: 0.2667\n",
      "Epoch 73/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2421 - accuracy: 0.9029 - val_loss: 4.5512 - val_accuracy: 0.2672\n",
      "Epoch 74/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2357 - accuracy: 0.9049 - val_loss: 4.6955 - val_accuracy: 0.2654\n",
      "Epoch 75/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2397 - accuracy: 0.9042 - val_loss: 4.6399 - val_accuracy: 0.2603\n",
      "Epoch 76/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2377 - accuracy: 0.9036 - val_loss: 4.6678 - val_accuracy: 0.2650\n",
      "Epoch 77/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2358 - accuracy: 0.9041 - val_loss: 4.6507 - val_accuracy: 0.2636\n",
      "Epoch 78/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2329 - accuracy: 0.9057 - val_loss: 4.7039 - val_accuracy: 0.2663\n",
      "Epoch 79/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2288 - accuracy: 0.9079 - val_loss: 4.7882 - val_accuracy: 0.2657\n",
      "Epoch 80/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2313 - accuracy: 0.9057 - val_loss: 4.7450 - val_accuracy: 0.2638\n",
      "Epoch 81/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2304 - accuracy: 0.9040 - val_loss: 4.7757 - val_accuracy: 0.2672\n",
      "Epoch 82/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.2290 - accuracy: 0.9065 - val_loss: 4.7708 - val_accuracy: 0.2653\n",
      "Epoch 83/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2271 - accuracy: 0.9068 - val_loss: 4.7702 - val_accuracy: 0.2617\n",
      "Epoch 84/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2275 - accuracy: 0.9065 - val_loss: 4.7993 - val_accuracy: 0.2657\n",
      "Epoch 85/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2276 - accuracy: 0.9056 - val_loss: 4.8151 - val_accuracy: 0.2619\n",
      "Epoch 86/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2276 - accuracy: 0.9066 - val_loss: 4.8094 - val_accuracy: 0.2615\n",
      "Epoch 87/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2223 - accuracy: 0.9089 - val_loss: 4.8475 - val_accuracy: 0.2631\n",
      "Epoch 88/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2204 - accuracy: 0.9072 - val_loss: 4.8502 - val_accuracy: 0.2638\n",
      "Epoch 89/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2209 - accuracy: 0.9087 - val_loss: 4.9122 - val_accuracy: 0.2623\n",
      "Epoch 90/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2183 - accuracy: 0.9090 - val_loss: 4.8351 - val_accuracy: 0.2644\n",
      "Epoch 91/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2161 - accuracy: 0.9086 - val_loss: 4.8597 - val_accuracy: 0.2630\n",
      "Epoch 92/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2174 - accuracy: 0.9100 - val_loss: 4.8850 - val_accuracy: 0.2657\n",
      "Epoch 93/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2163 - accuracy: 0.9097 - val_loss: 4.9025 - val_accuracy: 0.2642\n",
      "Epoch 94/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.2167 - accuracy: 0.9100 - val_loss: 4.8921 - val_accuracy: 0.2633\n",
      "Epoch 95/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2156 - accuracy: 0.9106 - val_loss: 4.9258 - val_accuracy: 0.2645\n",
      "Epoch 96/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2148 - accuracy: 0.9104 - val_loss: 4.9213 - val_accuracy: 0.2662\n",
      "Epoch 97/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2179 - accuracy: 0.9083 - val_loss: 4.9885 - val_accuracy: 0.2596\n",
      "Epoch 98/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2165 - accuracy: 0.9093 - val_loss: 4.9076 - val_accuracy: 0.2614\n",
      "Epoch 99/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2126 - accuracy: 0.9098 - val_loss: 4.9996 - val_accuracy: 0.2599\n",
      "Epoch 100/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.2118 - accuracy: 0.9098 - val_loss: 4.9613 - val_accuracy: 0.2583\n",
      "Epoch 101/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.2100 - accuracy: 0.9110 - val_loss: 4.9816 - val_accuracy: 0.2661\n",
      "Epoch 102/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.2105 - accuracy: 0.9106 - val_loss: 4.9596 - val_accuracy: 0.2623\n",
      "Epoch 103/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.2079 - accuracy: 0.9112 - val_loss: 5.0962 - val_accuracy: 0.2643\n",
      "Epoch 104/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2113 - accuracy: 0.9096 - val_loss: 5.0219 - val_accuracy: 0.2619\n",
      "Epoch 105/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2100 - accuracy: 0.9109 - val_loss: 5.0274 - val_accuracy: 0.2621\n",
      "Epoch 106/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2081 - accuracy: 0.9123 - val_loss: 4.9895 - val_accuracy: 0.2618\n",
      "Epoch 107/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2104 - accuracy: 0.9100 - val_loss: 5.0401 - val_accuracy: 0.2626\n",
      "Epoch 108/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2031 - accuracy: 0.9132 - val_loss: 5.1023 - val_accuracy: 0.2663\n",
      "Epoch 109/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.2058 - accuracy: 0.9112 - val_loss: 5.0839 - val_accuracy: 0.2595\n",
      "Epoch 110/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.2042 - accuracy: 0.9123 - val_loss: 5.0986 - val_accuracy: 0.2649\n",
      "Epoch 111/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.2057 - accuracy: 0.9100 - val_loss: 5.1206 - val_accuracy: 0.2637\n",
      "Epoch 112/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2043 - accuracy: 0.9114 - val_loss: 5.1460 - val_accuracy: 0.2655\n",
      "Epoch 113/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.2033 - accuracy: 0.9128 - val_loss: 5.0591 - val_accuracy: 0.2694\n",
      "Epoch 114/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2049 - accuracy: 0.9124 - val_loss: 5.0903 - val_accuracy: 0.2657\n",
      "Epoch 115/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2011 - accuracy: 0.9133 - val_loss: 5.1299 - val_accuracy: 0.2637\n",
      "Epoch 116/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.2000 - accuracy: 0.9147 - val_loss: 5.1154 - val_accuracy: 0.2688\n",
      "Epoch 117/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2015 - accuracy: 0.9122 - val_loss: 5.2003 - val_accuracy: 0.2611\n",
      "Epoch 118/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2018 - accuracy: 0.9127 - val_loss: 5.1517 - val_accuracy: 0.2683\n",
      "Epoch 119/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.2010 - accuracy: 0.9138 - val_loss: 5.2296 - val_accuracy: 0.2677\n",
      "Epoch 120/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.2029 - accuracy: 0.9110 - val_loss: 5.1730 - val_accuracy: 0.2665\n",
      "Epoch 121/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.2018 - accuracy: 0.9123 - val_loss: 5.2424 - val_accuracy: 0.2655\n",
      "Epoch 122/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.2007 - accuracy: 0.9121 - val_loss: 5.2382 - val_accuracy: 0.2661\n",
      "Epoch 123/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1995 - accuracy: 0.9111 - val_loss: 5.1914 - val_accuracy: 0.2666\n",
      "Epoch 124/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1982 - accuracy: 0.9135 - val_loss: 5.2378 - val_accuracy: 0.2613\n",
      "Epoch 125/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1980 - accuracy: 0.9117 - val_loss: 5.2427 - val_accuracy: 0.2644\n",
      "Epoch 126/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1969 - accuracy: 0.9146 - val_loss: 5.2183 - val_accuracy: 0.2645\n",
      "Epoch 127/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1966 - accuracy: 0.9139 - val_loss: 5.3498 - val_accuracy: 0.2643\n",
      "Epoch 128/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1947 - accuracy: 0.9140 - val_loss: 5.2798 - val_accuracy: 0.2637\n",
      "Epoch 129/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1948 - accuracy: 0.9138 - val_loss: 5.3117 - val_accuracy: 0.2631\n",
      "Epoch 130/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1949 - accuracy: 0.9152 - val_loss: 5.3389 - val_accuracy: 0.2619\n",
      "Epoch 131/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1961 - accuracy: 0.9145 - val_loss: 5.3191 - val_accuracy: 0.2642\n",
      "Epoch 132/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1944 - accuracy: 0.9148 - val_loss: 5.2630 - val_accuracy: 0.2615\n",
      "Epoch 133/250\n",
      "101/101 [==============================] - 11s 109ms/step - loss: 0.1961 - accuracy: 0.9133 - val_loss: 5.2558 - val_accuracy: 0.2647\n",
      "Epoch 134/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1952 - accuracy: 0.9139 - val_loss: 5.2356 - val_accuracy: 0.2645\n",
      "Epoch 135/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1934 - accuracy: 0.9160 - val_loss: 5.3389 - val_accuracy: 0.2607\n",
      "Epoch 136/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1949 - accuracy: 0.9140 - val_loss: 5.2545 - val_accuracy: 0.2650\n",
      "Epoch 137/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1941 - accuracy: 0.9150 - val_loss: 5.3295 - val_accuracy: 0.2650\n",
      "Epoch 138/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1952 - accuracy: 0.9144 - val_loss: 5.3900 - val_accuracy: 0.2625\n",
      "Epoch 139/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1908 - accuracy: 0.9151 - val_loss: 5.3676 - val_accuracy: 0.2618\n",
      "Epoch 140/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1925 - accuracy: 0.9125 - val_loss: 5.3442 - val_accuracy: 0.2609\n",
      "Epoch 141/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1922 - accuracy: 0.9139 - val_loss: 5.4097 - val_accuracy: 0.2629\n",
      "Epoch 142/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1921 - accuracy: 0.9136 - val_loss: 5.4081 - val_accuracy: 0.2618\n",
      "Epoch 143/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1925 - accuracy: 0.9133 - val_loss: 5.3702 - val_accuracy: 0.2635\n",
      "Epoch 144/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1939 - accuracy: 0.9132 - val_loss: 5.4239 - val_accuracy: 0.2635\n",
      "Epoch 145/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1906 - accuracy: 0.9144 - val_loss: 5.4026 - val_accuracy: 0.2602\n",
      "Epoch 146/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1905 - accuracy: 0.9148 - val_loss: 5.3845 - val_accuracy: 0.2645\n",
      "Epoch 147/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1912 - accuracy: 0.9155 - val_loss: 5.4537 - val_accuracy: 0.2607\n",
      "Epoch 148/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1888 - accuracy: 0.9161 - val_loss: 5.4380 - val_accuracy: 0.2630\n",
      "Epoch 149/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1898 - accuracy: 0.9153 - val_loss: 5.4012 - val_accuracy: 0.2627\n",
      "Epoch 150/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1881 - accuracy: 0.9144 - val_loss: 5.4686 - val_accuracy: 0.2594\n",
      "Epoch 151/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1895 - accuracy: 0.9146 - val_loss: 5.4538 - val_accuracy: 0.2685\n",
      "Epoch 152/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.1891 - accuracy: 0.9151 - val_loss: 5.4081 - val_accuracy: 0.2632\n",
      "Epoch 153/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1885 - accuracy: 0.9142 - val_loss: 5.4673 - val_accuracy: 0.2595\n",
      "Epoch 154/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1884 - accuracy: 0.9146 - val_loss: 5.4330 - val_accuracy: 0.2671\n",
      "Epoch 155/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1879 - accuracy: 0.9159 - val_loss: 5.4766 - val_accuracy: 0.2685\n",
      "Epoch 156/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1855 - accuracy: 0.9162 - val_loss: 5.4195 - val_accuracy: 0.2675\n",
      "Epoch 157/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1865 - accuracy: 0.9149 - val_loss: 5.4782 - val_accuracy: 0.2648\n",
      "Epoch 158/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1868 - accuracy: 0.9163 - val_loss: 5.5087 - val_accuracy: 0.2645\n",
      "Epoch 159/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1845 - accuracy: 0.9173 - val_loss: 5.4621 - val_accuracy: 0.2648\n",
      "Epoch 160/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1869 - accuracy: 0.9168 - val_loss: 5.4956 - val_accuracy: 0.2685\n",
      "Epoch 161/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1874 - accuracy: 0.9159 - val_loss: 5.5453 - val_accuracy: 0.2680\n",
      "Epoch 162/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1868 - accuracy: 0.9149 - val_loss: 5.4911 - val_accuracy: 0.2643\n",
      "Epoch 163/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.1849 - accuracy: 0.9157 - val_loss: 5.5660 - val_accuracy: 0.2681\n",
      "Epoch 164/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1846 - accuracy: 0.9157 - val_loss: 5.5011 - val_accuracy: 0.2651\n",
      "Epoch 165/250\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 0.1842 - accuracy: 0.9168 - val_loss: 5.6267 - val_accuracy: 0.2656\n",
      "Epoch 166/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1859 - accuracy: 0.9152 - val_loss: 5.5895 - val_accuracy: 0.2629\n",
      "Epoch 167/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1848 - accuracy: 0.9171 - val_loss: 5.5934 - val_accuracy: 0.2657\n",
      "Epoch 168/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1845 - accuracy: 0.9154 - val_loss: 5.5257 - val_accuracy: 0.2678\n",
      "Epoch 169/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1859 - accuracy: 0.9155 - val_loss: 5.5812 - val_accuracy: 0.2633\n",
      "Epoch 170/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1838 - accuracy: 0.9154 - val_loss: 5.6133 - val_accuracy: 0.2637\n",
      "Epoch 171/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1845 - accuracy: 0.9155 - val_loss: 5.5855 - val_accuracy: 0.2678\n",
      "Epoch 172/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1897 - accuracy: 0.9131 - val_loss: 5.6089 - val_accuracy: 0.2680\n",
      "Epoch 173/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1856 - accuracy: 0.9155 - val_loss: 5.5725 - val_accuracy: 0.2609\n",
      "Epoch 174/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1851 - accuracy: 0.9160 - val_loss: 5.6228 - val_accuracy: 0.2651\n",
      "Epoch 175/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1842 - accuracy: 0.9162 - val_loss: 5.6487 - val_accuracy: 0.2643\n",
      "Epoch 176/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1845 - accuracy: 0.9154 - val_loss: 5.6489 - val_accuracy: 0.2688\n",
      "Epoch 177/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1834 - accuracy: 0.9152 - val_loss: 5.6150 - val_accuracy: 0.2685\n",
      "Epoch 178/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1819 - accuracy: 0.9175 - val_loss: 5.6234 - val_accuracy: 0.2666\n",
      "Epoch 179/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1803 - accuracy: 0.9168 - val_loss: 5.5622 - val_accuracy: 0.2673\n",
      "Epoch 180/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1814 - accuracy: 0.9166 - val_loss: 5.5176 - val_accuracy: 0.2681\n",
      "Epoch 181/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1820 - accuracy: 0.9175 - val_loss: 5.6240 - val_accuracy: 0.2659\n",
      "Epoch 182/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1793 - accuracy: 0.9161 - val_loss: 5.6828 - val_accuracy: 0.2639\n",
      "Epoch 183/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1809 - accuracy: 0.9163 - val_loss: 5.7073 - val_accuracy: 0.2649\n",
      "Epoch 184/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1827 - accuracy: 0.9165 - val_loss: 5.6890 - val_accuracy: 0.2667\n",
      "Epoch 185/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1805 - accuracy: 0.9168 - val_loss: 5.6062 - val_accuracy: 0.2672\n",
      "Epoch 186/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.1820 - accuracy: 0.9157 - val_loss: 5.6585 - val_accuracy: 0.2638\n",
      "Epoch 187/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1803 - accuracy: 0.9167 - val_loss: 5.6775 - val_accuracy: 0.2671\n",
      "Epoch 188/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1807 - accuracy: 0.9157 - val_loss: 5.6211 - val_accuracy: 0.2653\n",
      "Epoch 189/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1817 - accuracy: 0.9167 - val_loss: 5.6817 - val_accuracy: 0.2683\n",
      "Epoch 190/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1781 - accuracy: 0.9175 - val_loss: 5.6953 - val_accuracy: 0.2671\n",
      "Epoch 191/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1804 - accuracy: 0.9182 - val_loss: 5.6848 - val_accuracy: 0.2654\n",
      "Epoch 192/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1806 - accuracy: 0.9157 - val_loss: 5.6953 - val_accuracy: 0.2647\n",
      "Epoch 193/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.1808 - accuracy: 0.9160 - val_loss: 5.6829 - val_accuracy: 0.2636\n",
      "Epoch 194/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1806 - accuracy: 0.9151 - val_loss: 5.6947 - val_accuracy: 0.2674\n",
      "Epoch 195/250\n",
      "101/101 [==============================] - 11s 109ms/step - loss: 0.1786 - accuracy: 0.9164 - val_loss: 5.7003 - val_accuracy: 0.2660\n",
      "Epoch 196/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1805 - accuracy: 0.9154 - val_loss: 5.7332 - val_accuracy: 0.2663\n",
      "Epoch 197/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1791 - accuracy: 0.9165 - val_loss: 5.6755 - val_accuracy: 0.2685\n",
      "Epoch 198/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1808 - accuracy: 0.9156 - val_loss: 5.7233 - val_accuracy: 0.2631\n",
      "Epoch 199/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1785 - accuracy: 0.9168 - val_loss: 5.7395 - val_accuracy: 0.2638\n",
      "Epoch 200/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1777 - accuracy: 0.9180 - val_loss: 5.7698 - val_accuracy: 0.2657\n",
      "Epoch 201/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1782 - accuracy: 0.9175 - val_loss: 5.7457 - val_accuracy: 0.2619\n",
      "Epoch 202/250\n",
      "101/101 [==============================] - 11s 104ms/step - loss: 0.1775 - accuracy: 0.9171 - val_loss: 5.7491 - val_accuracy: 0.2639\n",
      "Epoch 203/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1783 - accuracy: 0.9184 - val_loss: 5.8163 - val_accuracy: 0.2639\n",
      "Epoch 204/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1790 - accuracy: 0.9158 - val_loss: 5.8381 - val_accuracy: 0.2672\n",
      "Epoch 205/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1792 - accuracy: 0.9148 - val_loss: 5.8122 - val_accuracy: 0.2681\n",
      "Epoch 206/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1777 - accuracy: 0.9169 - val_loss: 5.7653 - val_accuracy: 0.2662\n",
      "Epoch 207/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1772 - accuracy: 0.9158 - val_loss: 5.7273 - val_accuracy: 0.2657\n",
      "Epoch 208/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1787 - accuracy: 0.9175 - val_loss: 5.7278 - val_accuracy: 0.2685\n",
      "Epoch 209/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1772 - accuracy: 0.9176 - val_loss: 5.7812 - val_accuracy: 0.2669\n",
      "Epoch 210/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1758 - accuracy: 0.9178 - val_loss: 5.7864 - val_accuracy: 0.2617\n",
      "Epoch 211/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1786 - accuracy: 0.9165 - val_loss: 5.7586 - val_accuracy: 0.2698\n",
      "Epoch 212/250\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 0.1792 - accuracy: 0.9146 - val_loss: 5.7881 - val_accuracy: 0.2684\n",
      "Epoch 213/250\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 0.1776 - accuracy: 0.9174 - val_loss: 5.8223 - val_accuracy: 0.2691\n",
      "Epoch 214/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1773 - accuracy: 0.9180 - val_loss: 5.7890 - val_accuracy: 0.2677\n",
      "Epoch 215/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1781 - accuracy: 0.9156 - val_loss: 5.8106 - val_accuracy: 0.2688\n",
      "Epoch 216/250\n",
      "101/101 [==============================] - 10s 104ms/step - loss: 0.1755 - accuracy: 0.9171 - val_loss: 5.7735 - val_accuracy: 0.2671\n",
      "Epoch 217/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1753 - accuracy: 0.9173 - val_loss: 5.7737 - val_accuracy: 0.2685\n",
      "Epoch 218/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1766 - accuracy: 0.9179 - val_loss: 5.8116 - val_accuracy: 0.2675\n",
      "Epoch 219/250\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 0.1768 - accuracy: 0.9179 - val_loss: 5.8194 - val_accuracy: 0.2667\n",
      "Epoch 220/250\n",
      "101/101 [==============================] - 12s 115ms/step - loss: 0.1762 - accuracy: 0.9173 - val_loss: 5.8408 - val_accuracy: 0.2660\n",
      "Epoch 221/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1751 - accuracy: 0.9176 - val_loss: 5.8209 - val_accuracy: 0.2667\n",
      "Epoch 222/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1755 - accuracy: 0.9167 - val_loss: 5.8072 - val_accuracy: 0.2666\n",
      "Epoch 223/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1755 - accuracy: 0.9185 - val_loss: 5.8187 - val_accuracy: 0.2681\n",
      "Epoch 224/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1748 - accuracy: 0.9179 - val_loss: 5.8649 - val_accuracy: 0.2661\n",
      "Epoch 225/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1762 - accuracy: 0.9165 - val_loss: 5.8282 - val_accuracy: 0.2683\n",
      "Epoch 226/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1761 - accuracy: 0.9179 - val_loss: 5.8489 - val_accuracy: 0.2661\n",
      "Epoch 227/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1740 - accuracy: 0.9184 - val_loss: 5.8612 - val_accuracy: 0.2656\n",
      "Epoch 228/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1750 - accuracy: 0.9164 - val_loss: 5.8720 - val_accuracy: 0.2645\n",
      "Epoch 229/250\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 0.1735 - accuracy: 0.9187 - val_loss: 5.9283 - val_accuracy: 0.2638\n",
      "Epoch 230/250\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 0.1748 - accuracy: 0.9178 - val_loss: 5.9213 - val_accuracy: 0.2629\n",
      "Epoch 231/250\n",
      "101/101 [==============================] - 11s 109ms/step - loss: 0.1781 - accuracy: 0.9158 - val_loss: 5.8800 - val_accuracy: 0.2630\n",
      "Epoch 232/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1748 - accuracy: 0.9171 - val_loss: 5.8862 - val_accuracy: 0.2671\n",
      "Epoch 233/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1737 - accuracy: 0.9181 - val_loss: 5.9903 - val_accuracy: 0.2663\n",
      "Epoch 234/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1755 - accuracy: 0.9165 - val_loss: 5.8957 - val_accuracy: 0.2688\n",
      "Epoch 235/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1754 - accuracy: 0.9167 - val_loss: 5.9396 - val_accuracy: 0.2623\n",
      "Epoch 236/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1758 - accuracy: 0.9185 - val_loss: 5.9649 - val_accuracy: 0.2645\n",
      "Epoch 237/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1747 - accuracy: 0.9175 - val_loss: 5.8944 - val_accuracy: 0.2642\n",
      "Epoch 238/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1751 - accuracy: 0.9185 - val_loss: 5.9116 - val_accuracy: 0.2618\n",
      "Epoch 239/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1766 - accuracy: 0.9169 - val_loss: 6.0339 - val_accuracy: 0.2623\n",
      "Epoch 240/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1758 - accuracy: 0.9161 - val_loss: 5.9345 - val_accuracy: 0.2656\n",
      "Epoch 241/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1749 - accuracy: 0.9183 - val_loss: 5.9693 - val_accuracy: 0.2672\n",
      "Epoch 242/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1749 - accuracy: 0.9184 - val_loss: 5.9451 - val_accuracy: 0.2661\n",
      "Epoch 243/250\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.1740 - accuracy: 0.9178 - val_loss: 5.9495 - val_accuracy: 0.2659\n",
      "Epoch 244/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1750 - accuracy: 0.9167 - val_loss: 5.9783 - val_accuracy: 0.2656\n",
      "Epoch 245/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1757 - accuracy: 0.9179 - val_loss: 5.9205 - val_accuracy: 0.2684\n",
      "Epoch 246/250\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 0.1724 - accuracy: 0.9178 - val_loss: 5.9788 - val_accuracy: 0.2691\n",
      "Epoch 247/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1735 - accuracy: 0.9181 - val_loss: 5.9511 - val_accuracy: 0.2668\n",
      "Epoch 248/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1737 - accuracy: 0.9187 - val_loss: 5.9695 - val_accuracy: 0.2620\n",
      "Epoch 249/250\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 0.1724 - accuracy: 0.9185 - val_loss: 5.9731 - val_accuracy: 0.2671\n",
      "Epoch 250/250\n",
      "101/101 [==============================] - 11s 108ms/step - loss: 0.1746 - accuracy: 0.9169 - val_loss: 5.9437 - val_accuracy: 0.2675\n"
     ]
    }
   ],
   "source": [
    "# bert_history = bert_model.fit(input_train, epochs=EPOCHS, validation_data=input_test)\n",
    "\n",
    "bert_history = bert_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size = BATCH_SIZE, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wrCv2zbD02yH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCZO0tlORmyW"
   },
   "outputs": [],
   "source": [
    "os.listdir('../../model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzYxDwyXRfb9"
   },
   "outputs": [],
   "source": [
    "# bert_model.save('../../model/friends_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JB_Yo7p-Rm_a"
   },
   "outputs": [],
   "source": [
    "# model.save('cnn.h5')\n",
    "# loaded_model = tf.keras.models.load_model('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCwhoNqu02yK",
    "outputId": "77e7e4d5-ebcc-42d3-b4ec-69e6c8e8d6c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.functional.Functional"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "uQ1_eUdc02yN"
   },
   "outputs": [],
   "source": [
    "# bert_model.save('../../model/friends_model.h5',save_format=\"tf\")\n",
    "bert_model.save_weights('friends_model_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBB0PIDYSZVD",
    "outputId": "fe42b66c-2645-44fc-e154-aa961a706090"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_113', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.modeling_tf_bert.TFBertForSequenceClassification at 0x7f414ec79a58>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_model = tf.keras.models.load_model('cnn.h5')\n",
    "\n",
    "test_load_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "test_load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "FLuiIDc-U3q2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "6l784O2xTNMT",
    "outputId": "914b4c8d-51cc-4505-c0a2-ad14bbefdf5e"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f271c8108c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_load_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'friends_model_weights1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_load_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2209\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2210\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2211\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2213\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    704\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    707\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"bert\" in the current model) was found to correspond to layer tf_bert_for_sequence_classification in the save file. However the new layer bert expects 199 weights, but the saved weights have 201 elements."
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-9, epsilon=1e-08)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metric = tf.keras.metrics.CategoricalCrossentropy('accuracy')\n",
    "\n",
    "test_load_model.load_weights('friends_model_weights1.h5')\n",
    "test_load_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHDTNVenTUmR"
   },
   "outputs": [],
   "source": [
    "bert_model.evaluate(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDH1kFo3Uq9q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QuRsiw_TYJj"
   },
   "outputs": [],
   "source": [
    "test_load_model.evaluate(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-mq2babThRC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J-Okj2NVbJT"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "SJmXJh0bYThb"
   },
   "outputs": [],
   "source": [
    "def map_to_dict_testing(input_ids, attention_masks, token_type_ids):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "    }\n",
    "\n",
    "def encode_test_dataset(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds[limit-1:limit]\n",
    "        print(ds)\n",
    "    \n",
    "    for text in ds.values:\n",
    "        bert_input = text_to_feature(text)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "    \n",
    "    print(len(ds.values))\n",
    "        \n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list)).map(map_to_dict_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "zd1Hk4mqVYhZ",
    "outputId": "78358e4e-8413-4a78-ace6-dd755663fc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14    and they weren't looking at you before\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9dd91854c4ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_test_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-a130c4210ce5>\u001b[0m in \u001b[0;36mencode_test_dataset\u001b[0;34m(ds, limit)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbert_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0minput_ids_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtoken_type_ids_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: text_to_feature() missing 1 required positional argument: 'max_seq'"
     ]
    }
   ],
   "source": [
    "data_test_sentence = encode_test_dataset(data_test['text'],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "WP-mlGNVe9fu",
    "outputId": "da42c3ea-8984-43f2-b443-bb8831a09494"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c'mon you're going out with the guy there's go...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wait does he eat chalk</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>then i look down and i realize there's a phone...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>never had that dream</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>and they weren't looking at you before</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "1   c'mon you're going out with the guy there's go...      4\n",
       "3                              wait does he eat chalk      5\n",
       "9   then i look down and i realize there's a phone...      3\n",
       "11                               never had that dream      4\n",
       "14             and they weren't looking at you before      2"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "79JoASfwXCKw",
    "outputId": "6a0a15d6-f07b-4e23-f359-e257c3567c6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {input_ids: (25,), token_type_ids: (25,), attention_mask: (25,)}, types: {input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7uYbBiZXCHA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcN1U1eIaN0D"
   },
   "outputs": [],
   "source": [
    "XX = test_load_model.predict(data_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "DvsdUQEjcGi4",
    "outputId": "37e25d29-38d6-465e-8213-a3d6712c0d1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42746884, -0.1117845 ,  0.1798155 , -0.21867178,  0.31221217,\n",
       "        0.2932364 , -0.16892028], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0bgWgl6cJQj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20200803_training_bert_TPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "p37",
   "language": "python",
   "name": "p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
